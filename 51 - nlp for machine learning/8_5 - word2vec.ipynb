{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c86f99c",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "- Word embeddings are a way to represent words as dense numerical vectors so that words with similar meanings have similar numerical representations.\n",
    "- Word embeddings convert words into numbers that capture meaning and relationships.\n",
    "\n",
    "### Vector with One Hot Encoding\n",
    "- \"cat\"  = [0, 1, 0, 0]\n",
    "- \"dog\"  = [0, 0, 1, 0]\n",
    "- Problem with OHE - → No relationship between cat and dog.\n",
    "\n",
    "### Vector with Word Embedddings\n",
    "- \"cat\"  → [0.12, -0.98, 0.45, ...]\n",
    "- \"dog\"  → [0.11, -0.95, 0.47, ...]\n",
    "- OHE problem of No relationship between cat and dog is solved with 'Word Embeddings' - ***Similar words have similar vectors***\n",
    "\n",
    "- <image src='./resources/images/word_embeddings_1.png' width=\"600\" height=\"400\"/>\n",
    "\n",
    "### Common Word Embedding Models\n",
    "- Word2Vec\n",
    "    - CBOW - Architecture #1 used in Word2Vec\n",
    "    - Skip-Gram - Architecture #2 used in Word2Vec\n",
    "- GloVe\n",
    "- FastText\n",
    "- Gensim\n",
    "\n",
    "- <image src='./resources/images/word_2_vec_architecture_types.png' width=\"500\" height=\"200\"/>\n",
    "\n",
    "\n",
    "### Dataset\n",
    "- The dataset used here has two columns\n",
    "- Input column = Email message\n",
    "- Output column = spam/ham : \n",
    "    - Spam = spam email message\n",
    "    - ham = non-spam email message\n",
    "\n",
    "### Word2Vec \n",
    "- Word2Vec is a technique used to learn word embeddings — numerical vector representations of words that capture semantic meaning and relationships between words.\n",
    "- It was introduced by Google in 2013.\n",
    "- Word2Vec input : corpus / documents / words\n",
    "- Word2Vec output: Vector matrix where two similar vectors will be close by in the graph, and dissimilar vectors will be far apart.\n",
    "- Google has created a 1.5 GB Model trained on 3 billion words which gives us a Vector Matrix ready to use.\n",
    "- Positive and Negative values in the Vector indicate opposite relationships e.g. boy and girl or king and queen, etc\n",
    "- Values closer to ZERO means no relationship or weak relationship\n",
    "- Values close to ONE means very strong relationship.\n",
    "- Word2Vec example Vector Matrix\n",
    "- <image src='./resources/images/word_2_vec_example_vector_matrix.png' width=\"500\" height=\"200\"/>\n",
    "\n",
    "### Cosine similarity and Word2Vec\n",
    "- 1) Cosine Similarity of similar vectors\n",
    "- <image src='./resources/images/cosine_similarity_of_similar_vectors.png' width=\"500\" height=\"200\"/>\n",
    "<br/>\n",
    "\n",
    "- 2) Cosine Similarity of dissimilar vectors\n",
    "- <image src='./resources/images/cosine_similarity_of_disimilar_vectors.png' width=\"500\" height=\"200\"/>\n",
    "<br/>\n",
    "\n",
    "- 3) Cosine Similarity of same vectors\n",
    "- <image src='./resources/images/cosine_similarity_of_same_vectors.png' width=\"500\" height=\"200\"/>\n",
    "<br/>\n",
    "\n",
    "- 4) Real life example (using movies) of similar vectors\n",
    "- <image src='./resources/images/movie_example_of_similar_vectors.png' width=\"500\" height=\"200\"/>\n",
    "\n",
    "\n",
    "### Advantages of Word2Vec\n",
    "- 1) Dense Vector Matrix\n",
    "    - With Word2Vec we get a Dense Matrix instead of Sparse matrix.\n",
    "    - ML algorithms can train in a a better way if we have a Dense Vector matrix.\n",
    "- 2) Semantic information getting captured\n",
    "    - We can also find the Cosine similarity between similar and dissimilar words\n",
    "    - Using this Cosine Similarity we can find similar words (e.g. honest and good) and also find dissimilar words (e.g. sad and happy)\n",
    "- 3) Vector Matrix Size is Manageable\n",
    "    - Earlier if we had a huge vocabulary size, we used to get a huge Vector Matrix size.\n",
    "    - But now, when we use the Google's pre-trained model for Word2Vec, we always get a fixed vector matrix size of 300 dimensions.\n",
    "- 4) Not Out of Vocabulary (OOV) \n",
    "    - Using Google's Word2Vec pre-trained model, we would cover all the word permutation - combinations, so we have a very less chance of getting any new Out of Vocabulary words.\n",
    "\n",
    "### AvgWord2Vec\n",
    "- AvgWord2Vec (Average Word2Vec) is a simple technique for converting an entire sentence or document into a single vector by averaging the Word2Vec embeddings of all words in that text.\n",
    "- Example\n",
    "    - Sentence = \"good food tasty\"\n",
    "    - Word embeddings = \n",
    "        - \"good\"  → [0.4, 0.6, 0.8]\n",
    "        - \"food\"  → [0.2, 0.5, 0.7]\n",
    "        - \"tasty\" → [0.6, 0.7, 0.9]\n",
    "    - Average = ([0.4,0.6,0.8] + [0.2,0.5,0.7] +[0.6,0.7,0.9]) / 3 = [0.4, 0.6, 0.8]\n",
    "    - The resulting vector represents the ***entire sentence/document***.\n",
    "\n",
    "### Google's Word2Vec and AvgWord2Vec\n",
    "- Google's Word2Vec converts ***each word*** in the Document into a 300 dimension vector.    \n",
    "- Using such large vectors for every word is too much to handle for any ML algorithm.\n",
    "- So we take an average the vectors of all the words in each sentence/document and get as output one vector for every sentence/document.\n",
    "\n",
    "***Vector creation WITHOUT using Average Word to Vec using Google's Word2Vec***\n",
    "- One 300 Dimension Vector for every word in the Sentence/Document\n",
    "- <image src='./resources/images/vector_without_average_word_to_vec_1.png' width=\"500\" height=\"200\"/>\n",
    " \n",
    "***Vector creation WITH using Average Word to Vec using Google's Word2Vec***\n",
    "- One 300 Dimension Vector for every Sentence/Document in the Corpus\n",
    "- <image src='./resources/images/vector_with_word_to_vec_1.png' width=\"500\" height=\"200\"/>\n",
    "\n",
    "\n",
    "### Process to follow for Word2Vec\n",
    "- Step-1) Load the dataset\n",
    "- Step-2) Feature Engineering\n",
    "     - Perform cleaning and text pre-processing of your corpus.\n",
    "- Step-3) Train Test Split\n",
    "    - Do a Train Test Split before you apply Bag of Words (BOW) and/or TF-IDF.\n",
    "    - This is because if you do BOW and/or TF-IDF then it will be applied on all of the data, test and train data.\n",
    "    - This will cause ***Data Leakage*** - which is when Test Data and Train Data are not totally unrelated. \n",
    "    - Doing Train Test Split before BOW and/or TF-IDF is a ***Best Practice***.\n",
    "- Step-4) Apply Bag of Words and/or TF-IDF (using Bag of Words here)\n",
    "- Step-5) Train the model and do Prediction using an ML algorithm \n",
    "- Step-6) Test the performance of the model (by using classification_report (here))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
