{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed1a6dbf",
   "metadata": {},
   "source": [
    "### Definitions\n",
    "- Corpus = Entire Paragraph\n",
    "- Documents = Sentences\n",
    "- Vocabulary = Unique words in the Corpus\n",
    "- Words = All words in the Corpus\n",
    "\n",
    "### Tokenization \n",
    "- Tokenization is a process in which we take a Corpus or Documents and convert them into tokens\n",
    "\n",
    "### Types of Tokenization\n",
    "- 1) Corpus to Words\n",
    "- 2) Documents to Words\n",
    "- 3) Corpus to Documents\n",
    "\n",
    "### Popular libraries for Tokenization\n",
    "- NLTK\n",
    "    - Open Source Library\n",
    "- Spacey \n",
    "    - spacey.io\n",
    "    - Open Source Library\n",
    "\n",
    "\n",
    "### Comparision : Corpus --- to ---> Word Tokenization Techniques\n",
    "<table>\n",
    "<tr>\n",
    "    <td></td>\n",
    "    <td>word_tokenize</td>\n",
    "    <td>wordpunct_tokenize</td>\n",
    "    <td>TreebankWordTokenizer</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>Punctuation behaviour</td>\n",
    "    <td>Splits punctuations into seperate tokens, e.g. commas/periods become seperate tokens (e.g.1) I'm --split_to--> I, 'm (2 tokens) (e.g.2) don't --split_to--> do, n't (2 tokens)</td>\n",
    "    <td>Splits punctuations into standalone tokens and breaks words around punctuations e.g.  I'm --split_to--> I, ', m (3 tokens)</td>\n",
    "    <td>Very close to word_tokenize (e.g) don't --split_to--> do, n't (2 tokens)</td>\n",
    "</tr>\n",
    "<tr>\n",
    "    <td>Treebank punctuation rules compliance</td>\n",
    "    <td>Follows Treebank punctuation rules</td>\n",
    "    <td>Does not follow Treebank punctuation rules</td>\n",
    "    <td>Follows Treebank punctuation rules</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "### Treebank punctuation rules\n",
    "- Treebank punctuation refers to the standardized set of punctuation tokenization rules defined by the Penn Treebank, a large, manually annotated corpus created for linguistic research in computational linguistics.\n",
    "\n",
    "- In practice, “Treebank punctuation” means how punctuation marks are separated, normalized, and represented as tokens so that text can be consistently parsed, tagged, and analyzed.\n",
    "\n",
    "- Rules\n",
    "    - Words and punctuation are separate tokens\n",
    "    - Contractions are systematically split\n",
    "    - Quotation marks and parentheses are normalized\n",
    "    - Parsing and POS tagging become deterministic\n",
    "    - These rules are implemented in NLTK’s TreebankWordTokenizer (and indirectly in word_tokenize)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33e63228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55299387",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus ='''\n",
    "Hello Welcome, to Ankur Israni's NLP tutorials. Please do watch the entire course! to become an expert in NLP.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab56af11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents =  [\"\\nHello Welcome, to Ankur Israni's NLP tutorials.\", 'Please do watch the entire course!', 'to become an expert in NLP.']\n",
      "type(documents) =  <class 'list'>\n",
      "Individual documents:\n",
      "document =  \n",
      "Hello Welcome, to Ankur Israni's NLP tutorials.\n",
      "document =  Please do watch the entire course!\n",
      "document =  to become an expert in NLP.\n"
     ]
    }
   ],
   "source": [
    "#A\n",
    "# Tokenization: 'Corpus to Documents' tokenization using 'sent_tokenize'\n",
    "# documents are sentences\n",
    "documents = sent_tokenize(corpus)\n",
    "print('documents = ', documents)\n",
    "print('type(documents) = ',type(documents))\n",
    "\n",
    "print('Individual documents:')\n",
    "for document in documents:\n",
    "    print('document = ',document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1908c4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words:  ['Hello', 'Welcome', ',', 'to', 'Ankur', 'Israni', \"'s\", 'NLP', 'tutorials', '.', 'Please', 'do', 'watch', 'the', 'entire', 'course', '!', 'to', 'become', 'an', 'expert', 'in', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "#B1\n",
    "# Tokenization: 'Corpus to Words' using 'word_tokenize'\n",
    "words = word_tokenize(corpus)\n",
    "print('words: ',words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "071eb03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words =  ['Hello', 'Welcome', ',', 'to', 'Ankur', 'Israni', \"'\", 's', 'NLP', 'tutorials', '.', 'Please', 'do', 'watch', 'the', 'entire', 'course', '!', 'to', 'become', 'an', 'expert', 'in', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "#B2\n",
    "# Tokenization: 'Corpus to Words' using 'wordpunct_tokenize'\n",
    "words = wordpunct_tokenize(corpus)\n",
    "print('words = ',words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858350e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words =  ['Hello', 'Welcome', ',', 'to', 'Ankur', 'Israni', \"'s\", 'NLP', 'tutorials.', 'Please', 'do', 'watch', 'the', 'entire', 'course', '!', 'to', 'become', 'an', 'expert', 'in', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "#B3\n",
    "# Tokenization: 'Corpus to Words' using TreebankWordTokenizer\n",
    "treeBankWordTokenizer = TreebankWordTokenizer()\n",
    "words = treeBankWordTokenizer.tokenize(corpus)\n",
    "print('words = ',words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a887db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word =  ['Hello', 'Welcome', ',', 'to', 'Ankur', 'Israni', \"'s\", 'NLP', 'tutorials', '.']\n",
      "word =  ['Please', 'do', 'watch', 'the', 'entire', 'course', '!']\n",
      "word =  ['to', 'become', 'an', 'expert', 'in', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "#C\n",
    "# Tokenization: 'Document to Words' using 'word_tokenize'\n",
    "for document in documents:\n",
    "    word = word_tokenize(document)\n",
    "    print(\"word = \",word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5856a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed37a9e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d0fab8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
