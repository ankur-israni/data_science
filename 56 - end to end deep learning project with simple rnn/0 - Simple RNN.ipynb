{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cb24d8e",
   "metadata": {},
   "source": [
    "**PCA**\n",
    "- PCA = Principal Component Analysis\n",
    "- It transforms a dataset with many features into a smaller set of new variables called principal components, while preserving as much variance (information) as possible.\n",
    "- Reduce high-dimensional data (e.g., 100 features → 10 features)\n",
    "- <img src=\"./resources/assets/pca_1.png\"></img>\n",
    "- PCA uses Cosine Similarity to display the co-related components (i.e. principal components) close together as shown in the graph above.\n",
    "- A valid usecase of Cosine Similarity is a Recommendation System e.g. If I am watching an action movie, I want more recommendations of another action movie.\n",
    "\n",
    "**Entropy**\n",
    "- In neural networks (especially classification), entropy measures uncertainty in predictions.\n",
    "- It comes from information theory.\n",
    "- Entropy quantifies how uncertain a probability distribution is.\n",
    "- H(p)=−∑p(x)logp(x)\n",
    "- High entropy → high uncertainty\n",
    "- Low entropy → confident prediction\n",
    "- If a model predicts:\n",
    "    -  0.5 / 0.5 → High entropy (very uncertain)\n",
    "    - 0.99 / 0.01 → Low entropy (very confident)\n",
    "- Entropy is used inside loss functions, in Neural Networks\n",
    "- Binary Cross-Entropy is used for Binary Classification\n",
    "    - Loss=−[ylog(p)+(1−y)log(1−p)]\n",
    "- Categorical Cross-Entropy is used for multi-class classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf824bb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
