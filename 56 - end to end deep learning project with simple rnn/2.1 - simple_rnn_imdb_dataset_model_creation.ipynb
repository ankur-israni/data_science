{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44eaed51",
   "metadata": {},
   "source": [
    "**Simple RNN Neural Network using the IMDB dataset**\n",
    "- Step 1) - Load the IMDB dataset\n",
    "- Step 2) Create the word index dictionary and the reverse word index dictionary\n",
    "- Step 3) Decode the first review - Convert from Numeric embeddings (word indices) to actual words\n",
    "- Step 4) Apply padding to x_train and x_test\n",
    "- Step 5) Create Simple RNN model\n",
    "- Step 5.1 Create the basic Simple RNN model\n",
    "- Step 5.2) Create The Early Stopping Callback\n",
    "- Step 5.3) Train the model and apply Early Stopping Callback\n",
    "- Step 5.4) Print Simple RNN Model metadata (Optional)\n",
    "- Step 6) Helper Functions\n",
    "- Step 7) Do prediction with a sample user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f266cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# IMDB dataset is a built-in dataset in TensorFlow that contains movie reviews along with their corresponding sentiment labels (positive or negative). IMDB dataset is present in other places too.\n",
    "from tensorflow.keras.datasets import imdb \n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "# Sequential API is a linear stack of layers in Keras, which allows you to create a neural network by adding layers sequentially. For any Neural Network 'Sequential' is a must.\n",
    "from tensorflow.keras.models import Sequential \n",
    "\n",
    "# Dense is for creating nodes which are connected on multiple sides (hence the term Dense) - Dense is frequently used to create the hidden layers and output layer of a neural network. \n",
    "# SimpleRNN will create the RNN nodes\n",
    "# Embedding is used to create the Embedding layer which is used to convert the integer encoded words into dense vectors of fixed size.\n",
    "from tensorflow.keras.layers import Embedding,SimpleRNN,Dense \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7208a0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test =  [list([1, 591, 202, 14, 31, 6, 717, 10, 10, 2, 2, 5, 4, 360, 7, 4, 177, 5760, 394, 354, 4, 123, 9, 1035, 1035, 1035, 10, 10, 13, 92, 124, 89, 488, 7944, 100, 28, 1668, 14, 31, 23, 27, 7479, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 2, 38, 32, 25, 7944, 451, 202, 14, 6, 717])\n",
      " list([1, 14, 22, 3443, 6, 176, 7, 5063, 88, 12, 2679, 23, 1310, 5, 109, 943, 4, 114, 9, 55, 606, 5, 111, 7, 4, 139, 193, 273, 23, 4, 172, 270, 11, 7216, 2, 4, 8463, 2801, 109, 1603, 21, 4, 22, 3861, 8, 6, 1193, 1330, 10, 10, 4, 105, 987, 35, 841, 2, 19, 861, 1074, 5, 1987, 2, 45, 55, 221, 15, 670, 5304, 526, 14, 1069, 4, 405, 5, 2438, 7, 27, 85, 108, 131, 4, 5045, 5304, 3884, 405, 9, 3523, 133, 5, 50, 13, 104, 51, 66, 166, 14, 22, 157, 9, 4, 530, 239, 34, 8463, 2801, 45, 407, 31, 7, 41, 3778, 105, 21, 59, 299, 12, 38, 950, 5, 4521, 15, 45, 629, 488, 2733, 127, 6, 52, 292, 17, 4, 6936, 185, 132, 1988, 5304, 1799, 488, 2693, 47, 6, 392, 173, 4, 2, 4378, 270, 2352, 4, 1500, 7, 4, 65, 55, 73, 11, 346, 14, 20, 9, 6, 976, 2078, 7, 5293, 861, 2, 5, 4182, 30, 3127, 2, 56, 4, 841, 5, 990, 692, 8, 4, 1669, 398, 229, 10, 10, 13, 2822, 670, 5304, 14, 9, 31, 7, 27, 111, 108, 15, 2033, 19, 7836, 1429, 875, 551, 14, 22, 9, 1193, 21, 45, 4829, 5, 45, 252, 8, 2, 6, 565, 921, 3639, 39, 4, 529, 48, 25, 181, 8, 67, 35, 1732, 22, 49, 238, 60, 135, 1162, 14, 9, 290, 4, 58, 10, 10, 472, 45, 55, 878, 8, 169, 11, 374, 5687, 25, 203, 28, 8, 818, 12, 125, 4, 3077])\n",
      " list([1, 111, 748, 4368, 1133, 2, 2, 4, 87, 1551, 1262, 7, 31, 318, 9459, 7, 4, 498, 5076, 748, 63, 29, 5161, 220, 686, 2, 5, 17, 12, 575, 220, 2507, 17, 6, 185, 132, 2, 16, 53, 928, 11, 2, 74, 4, 438, 21, 27, 2, 589, 8, 22, 107, 2, 2, 997, 1638, 8, 35, 2076, 9019, 11, 22, 231, 54, 29, 1706, 29, 100, 2, 2425, 34, 2, 8738, 2, 5, 2, 98, 31, 2122, 33, 6, 58, 14, 3808, 1638, 8, 4, 365, 7, 2789, 3761, 356, 346, 4, 2, 1060, 63, 29, 93, 11, 5421, 11, 2, 33, 6, 58, 54, 1270, 431, 748, 7, 32, 2580, 16, 11, 94, 2, 10, 10, 4, 993, 2, 7, 4, 1766, 2634, 2164, 2, 8, 847, 8, 1450, 121, 31, 7, 27, 86, 2663, 2, 16, 6, 465, 993, 2006, 2, 573, 17, 2, 42, 4, 2, 37, 473, 6, 711, 6, 8869, 7, 328, 212, 70, 30, 258, 11, 220, 32, 7, 108, 21, 133, 12, 9, 55, 465, 849, 3711, 53, 33, 2071, 1969, 37, 70, 1144, 4, 5940, 1409, 74, 476, 37, 62, 91, 1329, 169, 4, 1330, 2, 146, 655, 2212, 5, 258, 12, 184, 2, 546, 5, 849, 2, 7, 4, 22, 1436, 18, 631, 1386, 797, 7, 4, 8712, 71, 348, 425, 4320, 1061, 19, 2, 5, 2, 11, 661, 8, 339, 2, 4, 2455, 2, 7, 4, 1962, 10, 10, 263, 787, 9, 270, 11, 6, 9466, 4, 2, 2, 121, 4, 5437, 26, 4434, 19, 68, 1372, 5, 28, 446, 6, 318, 7149, 8, 67, 51, 36, 70, 81, 8, 4392, 2294, 36, 1197, 8, 2, 2, 18, 6, 711, 4, 9909, 26, 2, 1125, 11, 14, 636, 720, 12, 426, 28, 77, 776, 8, 97, 38, 111, 7489, 6175, 168, 1239, 5189, 137, 2, 18, 27, 173, 9, 2399, 17, 6, 2, 428, 2, 232, 11, 4, 8014, 37, 272, 40, 2708, 247, 30, 656, 6, 2, 54, 2, 3292, 98, 6, 2840, 40, 558, 37, 6093, 98, 4, 2, 1197, 15, 14, 9, 57, 4893, 5, 4659, 6, 275, 711, 7937, 2, 3292, 98, 6, 2, 10, 10, 6639, 19, 14, 2, 267, 162, 711, 37, 5900, 752, 98, 4, 2, 2378, 90, 19, 6, 2, 7, 2, 1810, 2, 4, 4770, 3183, 930, 8, 508, 90, 4, 1317, 8, 4, 2, 17, 2, 3965, 1853, 4, 1494, 8, 4468, 189, 4, 2, 6287, 5774, 4, 4770, 5, 95, 271, 23, 6, 7742, 6063, 2, 5437, 33, 1526, 6, 425, 3155, 2, 4535, 1636, 7, 4, 4669, 2, 469, 4, 4552, 54, 4, 150, 5664, 2, 280, 53, 2, 2, 18, 339, 29, 1978, 27, 7885, 5, 2, 68, 1830, 19, 6571, 2, 4, 1515, 7, 263, 65, 2132, 34, 6, 5680, 7489, 43, 159, 29, 9, 4706, 9, 387, 73, 195, 584, 10, 10, 1069, 4, 58, 810, 54, 14, 6078, 117, 22, 16, 93, 5, 1069, 4, 192, 15, 12, 16, 93, 34, 6, 1766, 2, 33, 4, 5673, 7, 15, 2, 9252, 3286, 325, 12, 62, 30, 776, 8, 67, 14, 17, 6, 2, 44, 148, 687, 2, 203, 42, 203, 24, 28, 69, 2, 6676, 11, 330, 54, 29, 93, 2, 21, 845, 2, 27, 1099, 7, 819, 4, 22, 1407, 17, 6, 2, 787, 7, 2460, 2, 2, 100, 30, 4, 3737, 3617, 3169, 2321, 42, 1898, 11, 4, 3814, 42, 101, 704, 7, 101, 999, 15, 1625, 94, 2926, 180, 5, 9, 9101, 34, 2, 45, 6, 1429, 22, 60, 6, 1220, 31, 11, 94, 6408, 96, 21, 94, 749, 9, 57, 975])\n",
      " ...\n",
      " list([1, 13, 1408, 15, 8, 135, 14, 9, 35, 32, 46, 394, 20, 62, 30, 5093, 21, 45, 184, 78, 4, 1492, 910, 769, 2290, 2515, 395, 4257, 5, 1454, 11, 119, 2, 89, 1036, 4, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 7, 185, 2280, 284, 1842, 2, 37, 315, 4, 226, 20, 272, 2942, 40, 29, 152, 60, 181, 8, 30, 50, 553, 362, 80, 119, 12, 21, 846, 5518])\n",
      " list([1, 11, 119, 241, 9, 4, 840, 20, 12, 468, 15, 94, 3684, 562, 791, 39, 4, 86, 107, 8, 97, 14, 31, 33, 4, 2960, 7, 743, 46, 1028, 9, 3531, 5, 4, 768, 47, 8, 79, 90, 145, 164, 162, 50, 6, 501, 119, 7, 9, 4, 78, 232, 15, 16, 224, 11, 4, 333, 20, 4, 985, 200, 5, 2, 5, 9, 1861, 8, 79, 357, 4, 20, 47, 220, 57, 206, 139, 11, 12, 5, 55, 117, 212, 13, 1276, 92, 124, 51, 45, 1188, 71, 536, 13, 520, 14, 20, 6, 2302, 7, 470])\n",
      " list([1, 6, 52, 7465, 430, 22, 9, 220, 2594, 8, 28, 2, 519, 3227, 6, 769, 15, 47, 6, 3482, 4067, 8, 114, 5, 33, 222, 31, 55, 184, 704, 5586, 2, 19, 346, 3153, 5, 6, 364, 350, 4, 184, 5586, 9, 133, 1810, 11, 5417, 2, 21, 4, 7298, 2, 570, 50, 2005, 2643, 9, 6, 1249, 17, 6, 2, 2, 21, 17, 6, 1211, 232, 1138, 2249, 29, 266, 56, 96, 346, 194, 308, 9, 194, 21, 29, 218, 1078, 19, 4, 78, 173, 7, 27, 2, 5698, 3406, 718, 2, 9, 6, 6907, 17, 210, 5, 3281, 5677, 47, 77, 395, 14, 172, 173, 18, 2740, 2931, 4517, 82, 127, 27, 173, 11, 6, 392, 217, 21, 50, 9, 57, 65, 12, 2, 53, 40, 35, 390, 7, 11, 4, 3567, 7, 4, 314, 74, 6, 792, 22, 2, 19, 714, 727, 5205, 382, 4, 91, 6533, 439, 19, 14, 20, 9, 1441, 5805, 1118, 4, 756, 25, 124, 4, 31, 12, 16, 93, 804, 34, 2005, 2643])]\n",
      "Training data shape: (25000,), Training labels shape: (25000,)\n",
      "Testing data shape: (25000,), Testing labels shape: (25000,)\n",
      "First review (as word indices): [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "First review output: 1\n"
     ]
    }
   ],
   "source": [
    "# Step 1 - Load the IMDB dataset\n",
    "# The imdb.load_data() gives the word embeddings (in the form of numerical values) for the reviews in the dataset.\n",
    "# - max_vocabulary_size = 10000 means that we will only consider the top 10,000 most frequently occurring words in the reviews and ignore the rest of the words.\n",
    "max_vocabulary_size=10000 \n",
    "(x_train,y_train),(x_test,y_test)=imdb.load_data(num_words=max_vocabulary_size)\n",
    "print('x_test = ',x_test)\n",
    "\n",
    "# Print the shape of the data\n",
    "print(f'Training data shape: {x_train.shape}, Training labels shape: {y_train.shape}')\n",
    "print(f'Testing data shape: {x_test.shape}, Testing labels shape: {y_test.shape}')\n",
    "\n",
    "# Print the first review and its corresponding label\n",
    "# - x_train and x_test - Each number in the list (x_train or x_test)is a 'numeric word index' that corresponds to a specific word in the IMDB dataset's vocabulary.\n",
    "# - y_train and y_test - Each number in the list (y_train or y_test) is a label that indicates whether the review is positive (1) or negative (0).\n",
    "# - x_train[0] → first review\n",
    "# - x_train[1] → second review\n",
    "sample_first_review=x_train[0]\n",
    "sample_first_review_output=y_train[0]\n",
    "print(f'First review (as word indices): {sample_first_review}')\n",
    "print(f'First review output: {sample_first_review_output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "efd05d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_index = \n",
      "[('fawn', 34701), ('tsukino', 52006), ('nunnery', 52007), ('sonja', 16816), ('vani', 63951), ('woods', 1408), ('spiders', 16115), ('hanging', 2345), ('woody', 2289), ('trawling', 52008)]\n",
      "reverse_word_index = \n",
      "[(34701, 'fawn'), (52006, 'tsukino'), (52007, 'nunnery'), (16816, 'sonja'), (63951, 'vani'), (1408, 'woods'), (16115, 'spiders'), (2345, 'hanging'), (2289, 'woody'), (52008, 'trawling')]\n"
     ]
    }
   ],
   "source": [
    "# Step 2) Create the word index dictionary and the reverse word index dictionary\n",
    "# word_index = e.g. fawn: 34701 means that the word 'fawn' is represented by the index 34701 in the IMDB dataset's vocabulary.\n",
    "# reverse_word_index (opp of word_index)= e.g. 34701: fawn means that the word 'fawn' is represented by the index 34701 in the IMDB dataset's vocabulary.\n",
    "word_index=imdb.get_word_index()\n",
    "print('word_index = ')\n",
    "print(list(word_index.items())[:10]) # Print the first 10 items in the word_index dictionary\n",
    "\n",
    "reverse_word_index = {value: key for key, value in word_index.items()}\n",
    "print('reverse_word_index = ')\n",
    "print(list(reverse_word_index.items())[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "885ffcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded first review: ? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "# Step 3) Decode the first review - Convert from Numeric embeddings (word indices) to actual words\n",
    "# This is an optional step just to see the decoding (Numeric embedding --> Text format) actual works \n",
    "# - The word indices in the sample_first_review are offset by 3 because the IMDB dataset reserves the indices 0, 1, and 2 for special tokens (padding, start\n",
    "decoded_first_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in sample_first_review])\n",
    "print('Decoded first review:', decoded_first_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf220d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before padding shape =  (25000,)\n",
      "Before padding:  [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "after padding shape =  (25000, 500)\n",
      "after padding:  [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    1   14   22   16   43  530  973 1622 1385   65  458 4468\n",
      "   66 3941    4  173   36  256    5   25  100   43  838  112   50  670\n",
      "    2    9   35  480  284    5  150    4  172  112  167    2  336  385\n",
      "   39    4  172 4536 1111   17  546   38   13  447    4  192   50   16\n",
      "    6  147 2025   19   14   22    4 1920 4613  469    4   22   71   87\n",
      "   12   16   43  530   38   76   15   13 1247    4   22   17  515   17\n",
      "   12   16  626   18    2    5   62  386   12    8  316    8  106    5\n",
      "    4 2223 5244   16  480   66 3785   33    4  130   12   16   38  619\n",
      "    5   25  124   51   36  135   48   25 1415   33    6   22   12  215\n",
      "   28   77   52    5   14  407   16   82    2    8    4  107  117 5952\n",
      "   15  256    4    2    7 3766    5  723   36   71   43  530  476   26\n",
      "  400  317   46    7    4    2 1029   13  104   88    4  381   15  297\n",
      "   98   32 2071   56   26  141    6  194 7486   18    4  226   22   21\n",
      "  134  476   26  480    5  144   30 5535   18   51   36   28  224   92\n",
      "   25  104    4  226   65   16   38 1334   88   12   16  283    5   16\n",
      " 4472  113  103   32   15   16 5345   19  178   32]\n"
     ]
    }
   ],
   "source": [
    "# Step 4 - Apply padding to x_train and x_test\n",
    "# - The length of each review (i.e. the number of words in each review) can vary, \n",
    "#- which is why we need to pad the sequences to ensure that they all have \n",
    "# - the same length (max_len=500 in this case).\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "max_len=500\n",
    "\n",
    "'''\n",
    "Output before padding:\n",
    "= (25000,) \n",
    "- This means that there are 25,000 reviews in the training set \n",
    "- and each review is represented as a list of word indices (numeric embeddings). \n",
    "- The length of each review (i.e. the number of words in each review) can vary, \n",
    "- which is why we need to pad the sequences to ensure that they all have \n",
    "- the same length (max_len=500 in this case).\n",
    "'''\n",
    "print('Before padding shape = ',x_train.shape) \n",
    "print('Before padding: ',x_train[0])\n",
    "\n",
    "\n",
    "# Padding\n",
    "# By default, pad_sequences() function does pre-padding, which means that it adds zeros at the beginning of the sequences.\n",
    "x_train=sequence.pad_sequences(x_train,maxlen=max_len)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
    "\n",
    "'''\n",
    "Output after padding:\n",
    "- (25000, 500)\n",
    "'''\n",
    "# Output \n",
    "print('after padding shape = ',x_train.shape)\n",
    "print('after padding: ',x_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f64f30a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)        │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ simple_rnn_2 (\u001b[38;5;33mSimpleRNN\u001b[0m)        │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 5 - Create Simple RNN model\n",
    "# Step 5.1 - Create the basic Simple RNN model\n",
    "simple_rnn_model=Sequential()\n",
    "\n",
    "# Embedding Layers - This layer will take the word indices (numeric embeddings) as input and convert them into dense vectors of fixed size (128 in this case).\n",
    "# 128 neurons in the embedding layer.\n",
    "simple_rnn_model.add(Embedding(max_vocabulary_size ,128,input_length=max_len)) \n",
    "\n",
    "simple_rnn_model.add(SimpleRNN(128,activation='relu')) # Simple RNN layer with 128 units and ReLU activation function\n",
    "\n",
    "# Dense layer with 1 neuron and sigmoid activation function for binary classification\n",
    "simple_rnn_model.add(Dense(1,activation=\"sigmoid\")) # Dense layer with 1 neuron and sigmoid activation function for binary classification\n",
    "\n",
    "# Binary Crossentropy loss function is used for binary classification problems, where the output is either 0 or 1.\n",
    "simple_rnn_model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "simple_rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "14115907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.early_stopping.EarlyStopping at 0x1873dc500>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5.2) Create The Early Stopping Callback\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "earlystopping=EarlyStopping(monitor='val_loss',patience=5,restore_best_weights=True)\n",
    "earlystopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60634ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 91ms/step - accuracy: 0.6014 - loss: 870.0620 - val_accuracy: 0.6620 - val_loss: 0.6169\n",
      "Epoch 2/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 168ms/step - accuracy: 0.7398 - loss: 0.5141 - val_accuracy: 0.7358 - val_loss: 0.5209\n",
      "Epoch 3/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 176ms/step - accuracy: 0.8306 - loss: 0.3792 - val_accuracy: 0.7710 - val_loss: 0.4863\n",
      "Epoch 4/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 157ms/step - accuracy: 0.8267 - loss: 20717030.0000 - val_accuracy: 0.8040 - val_loss: 0.4426\n",
      "Epoch 5/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 173ms/step - accuracy: 0.9004 - loss: 0.2516 - val_accuracy: 0.7978 - val_loss: 0.4620\n",
      "Epoch 6/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 199ms/step - accuracy: 0.9171 - loss: 0.2318 - val_accuracy: 0.8066 - val_loss: 0.4597\n",
      "Epoch 7/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 174ms/step - accuracy: 0.9240 - loss: 0.2043 - val_accuracy: 0.8090 - val_loss: 0.5734\n",
      "Epoch 8/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 174ms/step - accuracy: 0.9474 - loss: 0.1433 - val_accuracy: 0.7954 - val_loss: 0.5791\n",
      "Epoch 9/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 186ms/step - accuracy: 0.9583 - loss: 0.1162 - val_accuracy: 0.8038 - val_loss: 0.6391\n",
      "Training history =  {'accuracy': [0.6013500094413757, 0.7397500276565552, 0.8305500149726868, 0.8266500234603882, 0.9003999829292297, 0.9171000123023987, 0.9240000247955322, 0.9474499821662903, 0.958299994468689], 'loss': [870.06201171875, 0.5141280293464661, 0.3792036175727844, 20717030.0, 0.25162988901138306, 0.23178964853286743, 0.20427997410297394, 0.14325560629367828, 0.11623287200927734], 'val_accuracy': [0.6620000004768372, 0.73580002784729, 0.7710000276565552, 0.8040000200271606, 0.7978000044822693, 0.8065999746322632, 0.8090000152587891, 0.7954000234603882, 0.8037999868392944], 'val_loss': [0.6169074177742004, 0.5208524465560913, 0.4863264560699463, 0.442626416683197, 0.46202754974365234, 0.45969295501708984, 0.5734408497810364, 0.5790925025939941, 0.6390517950057983]}\n"
     ]
    }
   ],
   "source": [
    "# Step 5.3) Train the model and apply Early Stopping Callback\n",
    "'''\n",
    "- When you call .fit(), Keras trains the model and returns a history object \n",
    "- that stores the training results from your model.\n",
    "- It stores metrics for each epoch, such as:\n",
    "    - Training loss\n",
    "    - Training accuracy\n",
    "    - Validation loss\n",
    "    - Validation accuracy\n",
    "- We can access this information through history.history, which is a dictionary containing the metrics for each epoch.\n",
    "'''\n",
    "history=simple_rnn_model.fit(\n",
    "    x_train,y_train,epochs=10,batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[earlystopping]\n",
    ")\n",
    "print('Training history = ',history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14797235",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40159227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights =  [array([[-0.37726715, -0.32035768, -0.20062667, ...,  0.4667506 ,\n",
      "        -0.11329959, -1.0732812 ],\n",
      "       [ 0.07108647,  0.03172462, -0.01864558, ..., -0.04093852,\n",
      "        -0.06434023, -0.00741006],\n",
      "       [-0.0056583 ,  0.0506408 ,  0.05242374, ...,  0.00637565,\n",
      "         0.0645816 , -0.05299132],\n",
      "       ...,\n",
      "       [-0.02541414, -0.06369624, -0.06624307, ...,  0.05469832,\n",
      "         0.04099702, -0.05451935],\n",
      "       [ 0.06334846,  0.04406402,  0.0189462 , ..., -0.06024647,\n",
      "        -0.04491138,  0.01217934],\n",
      "       [-0.04668416, -0.07016868, -0.12000916, ...,  0.04813214,\n",
      "         0.09461384, -0.05623053]], dtype=float32), array([[ 0.02728274, -0.147261  ,  0.03197928, ...,  0.12192511,\n",
      "         0.15190785, -0.04132107],\n",
      "       [ 0.13361727,  0.09165725, -0.22814906, ..., -0.00184063,\n",
      "         0.11813181,  0.00085336],\n",
      "       [-0.005473  ,  0.08858676, -0.15086651, ..., -0.03359535,\n",
      "        -0.04997173,  0.03973466],\n",
      "       ...,\n",
      "       [-0.14052275, -0.03455053,  0.09607219, ..., -0.08067878,\n",
      "        -0.0028425 , -0.20755747],\n",
      "       [ 0.01236082,  0.15528716,  0.24399026, ..., -0.01478111,\n",
      "         0.05938114, -0.02157366],\n",
      "       [ 0.13451067,  0.08443466, -0.07107092, ...,  0.1273894 ,\n",
      "        -0.16645932, -0.05253803]], dtype=float32), array([[-0.04129015,  0.02502974,  0.02081948, ..., -0.06148918,\n",
      "         0.00527354,  0.10224074],\n",
      "       [-0.06223093, -0.12963332,  0.06968877, ..., -0.02014206,\n",
      "        -0.13052018,  0.08481596],\n",
      "       [ 0.02430964, -0.12976217,  0.05619985, ..., -0.1077857 ,\n",
      "         0.02876602, -0.04146132],\n",
      "       ...,\n",
      "       [-0.25921497, -0.02074935,  0.74674624, ..., -0.24661721,\n",
      "        -0.05128289, -0.05984383],\n",
      "       [-0.11904678, -0.04122769,  0.11601944, ..., -0.03179809,\n",
      "         0.09867991,  0.05166182],\n",
      "       [ 0.04021882,  0.11192317, -0.03072182, ..., -0.15130965,\n",
      "         0.09773946, -0.02125382]], dtype=float32), array([-0.01713513, -0.02646592,  0.03354702, -0.06083579, -0.01947537,\n",
      "       -0.0612064 , -0.01756564, -0.05765685,  0.0102771 , -0.02510255,\n",
      "        0.01084386, -0.04585104, -0.06066808,  0.04319071, -0.02826945,\n",
      "        0.00928632, -0.02665547,  0.02785501, -0.02062919, -0.02095424,\n",
      "       -0.04434792,  0.01985905, -0.00593934,  0.01449371, -0.05569055,\n",
      "       -0.05982404, -0.03288902, -0.03516234, -0.05835379, -0.0342575 ,\n",
      "        0.04485307, -0.01789151, -0.04696518, -0.05408701,  0.01377941,\n",
      "        0.01058506,  0.02019251, -0.04407724,  0.11299291,  0.02853604,\n",
      "       -0.03924139,  0.00864842, -0.03538332, -0.03897144, -0.04146971,\n",
      "        0.00825196,  0.02767149,  0.04370027, -0.04131977, -0.01825209,\n",
      "       -0.05100999,  0.03769236, -0.0030448 ,  0.00841725, -0.0361581 ,\n",
      "       -0.0154546 ,  0.0460964 , -0.05334425,  0.00847473, -0.05934212,\n",
      "       -0.01533736, -0.03517443, -0.0406963 , -0.01612416,  0.00579104,\n",
      "        0.01020374, -0.02387456, -0.02955491, -0.0390228 , -0.05795687,\n",
      "       -0.03695267, -0.00342108,  0.01292167,  0.03871764,  0.00939121,\n",
      "       -0.05270404,  0.01905835, -0.01291321, -0.06556839,  0.06041318,\n",
      "       -0.01119564,  0.02907683, -0.03533368, -0.01758627, -0.06575781,\n",
      "        0.01315404, -0.0606786 , -0.0468162 , -0.04179571, -0.01574091,\n",
      "        0.0286412 ,  0.01018121, -0.03239476, -0.01890688, -0.05183809,\n",
      "        0.00455346,  0.01390599, -0.0383324 , -0.05238924, -0.05563089,\n",
      "       -0.0164705 ,  0.00338643, -0.07266332, -0.0212015 , -0.05065967,\n",
      "        0.00951424, -0.00701645, -0.02769894,  0.05777685, -0.03653643,\n",
      "        0.00141677, -0.04882973,  0.03579892,  0.02740872, -0.02365249,\n",
      "       -0.01327121, -0.00631656, -0.04273745, -0.01875823, -0.02095719,\n",
      "       -0.03159232,  0.00615398, -0.04338237, -0.029838  , -0.03673036,\n",
      "       -0.03189145,  0.02803287,  0.00800489], dtype=float32), array([[-0.02311645],\n",
      "       [ 0.21283959],\n",
      "       [-0.08071901],\n",
      "       [ 0.13896413],\n",
      "       [ 0.05914129],\n",
      "       [-0.04774375],\n",
      "       [-0.01394806],\n",
      "       [-0.19584468],\n",
      "       [-0.01031753],\n",
      "       [-0.08129724],\n",
      "       [ 0.06229278],\n",
      "       [-0.07591128],\n",
      "       [-0.25532994],\n",
      "       [-0.19295667],\n",
      "       [ 0.02346418],\n",
      "       [-0.03869037],\n",
      "       [ 0.17657578],\n",
      "       [ 0.09938399],\n",
      "       [ 0.20737661],\n",
      "       [ 0.11453564],\n",
      "       [ 0.14192991],\n",
      "       [ 0.05702535],\n",
      "       [ 0.12614168],\n",
      "       [ 0.1352464 ],\n",
      "       [ 0.15524316],\n",
      "       [ 0.19171584],\n",
      "       [ 0.20593596],\n",
      "       [ 0.09157907],\n",
      "       [-0.01609895],\n",
      "       [-0.13370562],\n",
      "       [ 0.11172587],\n",
      "       [-0.10499351],\n",
      "       [-0.07534675],\n",
      "       [ 0.10661187],\n",
      "       [ 0.04137502],\n",
      "       [ 0.12763035],\n",
      "       [ 0.20767538],\n",
      "       [-0.01725036],\n",
      "       [-0.21308884],\n",
      "       [-0.00909092],\n",
      "       [ 0.12578687],\n",
      "       [ 0.08630741],\n",
      "       [ 0.04360463],\n",
      "       [ 0.2445432 ],\n",
      "       [ 0.14494649],\n",
      "       [ 0.08243424],\n",
      "       [ 0.21539237],\n",
      "       [ 0.16904064],\n",
      "       [ 0.0124271 ],\n",
      "       [-0.25284895],\n",
      "       [ 0.15669218],\n",
      "       [ 0.19724137],\n",
      "       [ 0.16141553],\n",
      "       [ 0.218806  ],\n",
      "       [ 0.3089289 ],\n",
      "       [ 0.01686622],\n",
      "       [ 0.45694515],\n",
      "       [-0.0194918 ],\n",
      "       [ 0.14616801],\n",
      "       [ 0.12893432],\n",
      "       [ 0.18025856],\n",
      "       [-0.01564013],\n",
      "       [-0.03885433],\n",
      "       [ 0.63156253],\n",
      "       [-0.17088929],\n",
      "       [-0.4826778 ],\n",
      "       [-0.18355927],\n",
      "       [ 0.30189916],\n",
      "       [ 0.18682232],\n",
      "       [-0.27651107],\n",
      "       [ 0.02120355],\n",
      "       [-0.17759013],\n",
      "       [-0.18956289],\n",
      "       [-0.00337391],\n",
      "       [ 0.10344076],\n",
      "       [ 0.42223248],\n",
      "       [ 0.01101523],\n",
      "       [-0.15172118],\n",
      "       [ 0.05185138],\n",
      "       [-0.08848929],\n",
      "       [ 0.16231333],\n",
      "       [ 0.61808264],\n",
      "       [-0.27280042],\n",
      "       [ 0.20946905],\n",
      "       [-0.01315748],\n",
      "       [ 0.01134632],\n",
      "       [ 0.12400485],\n",
      "       [ 0.16389772],\n",
      "       [ 0.05730321],\n",
      "       [-0.06328686],\n",
      "       [ 0.1227833 ],\n",
      "       [ 0.03967353],\n",
      "       [ 0.10377114],\n",
      "       [-0.00377628],\n",
      "       [ 0.13343921],\n",
      "       [ 0.10418615],\n",
      "       [ 0.05006281],\n",
      "       [-0.05439043],\n",
      "       [-0.4894935 ],\n",
      "       [-0.03562927],\n",
      "       [ 0.18331468],\n",
      "       [-0.4715271 ],\n",
      "       [ 0.13694385],\n",
      "       [-0.23478584],\n",
      "       [ 0.29706386],\n",
      "       [ 0.21556607],\n",
      "       [-0.1942177 ],\n",
      "       [-0.22917826],\n",
      "       [ 0.20285943],\n",
      "       [ 0.13430649],\n",
      "       [-0.03082958],\n",
      "       [-0.23853825],\n",
      "       [-0.13228928],\n",
      "       [-0.06591465],\n",
      "       [ 0.06943265],\n",
      "       [ 0.35637772],\n",
      "       [ 0.00439038],\n",
      "       [ 0.03902883],\n",
      "       [-0.08575143],\n",
      "       [-0.02376498],\n",
      "       [ 0.15845053],\n",
      "       [ 0.01083693],\n",
      "       [-0.01335709],\n",
      "       [ 0.19407177],\n",
      "       [ 0.11308746],\n",
      "       [-0.13464181],\n",
      "       [ 0.00337762],\n",
      "       [-0.02271894]], dtype=float32), array([0.5690741], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# Step 5.4) Print Simple RNN Model metadata (Optional)\n",
    "# Print model weights\n",
    "# - The get_weights() method of a Keras model returns a list of all the weights \n",
    "# - (1) Embedding weights\n",
    "# - (2) Simple RNN weights\n",
    "# - (3) Bias\n",
    "# - (4) Dense layer weights\n",
    "# - (5) Recurrent weights of the Simple RNN layer\n",
    "\n",
    "# simple_rnn_model.get_weights() - Returns the weights for all layers in order of the layers in the model.\n",
    "# simple_rnn_model.layer.get_weights()[0] - Returns the weights for a specific layer in the model. You can specify the layer by its index or name.\n",
    "print('Model weights = ',simple_rnn_model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00dc2bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fd302bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Step 5.5) Save the Simple RNN model\n",
    "simple_rnn_model.save('./resources/dist/2_simple_rnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbcb2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Helper Functions\n",
    "\n",
    "# Function to convert numeric embeddings (word indices) back to text format\n",
    "# e.g. Input = encoded_review = [1, 14, 22, 16, 43]\n",
    "# Output = \"? this film was great\"\n",
    "'''\n",
    " - i - 3 → adjusts for IMDB’s index offset\n",
    " - reverse_word_index → maps number → word\n",
    " - '?' → used if word not found\n",
    "\n",
    " This function is not used in the example.\n",
    "'''\n",
    "def decode_review(encoded_review):\n",
    "    return ' '.join([reverse_word_index.get(i - 3, '?') for i in encoded_review])\n",
    "\n",
    "# Function to preprocess user input \n",
    "# - Split the whole text into words\n",
    "# - Convert the words to their corresponding word indices (numeric embeddings) using the word_index dictionary\n",
    "# - Pad the encoded review to ensure it has the same length as the reviews in the training data (max_len=500 in this case).\n",
    "'''\n",
    "e.g. Input = text = \"This movie was great\"\n",
    "e.g. Output = [[1, 14, 22, 16, 43, 0, 0, 0, ..., 0]] (padded and encoded review)\n",
    "'''\n",
    "def preprocess_text(text):\n",
    "    words = text.lower().split() # Split the input text to words\n",
    "    encoded_review = [word_index.get(word, 2) + 3 for word in words] # Convert the words to their corresponding word indices (numeric embeddings) using the word_index dictionary. If a word is not found in the word_index, it is assigned a default index of 2 (which corresponds to the \"unknown\" token in the IMDB dataset).\n",
    "    padded_review = sequence.pad_sequences([encoded_review], maxlen=500) # Pad the encoded review to ensure it has the same length as the reviews in the training data (max_len=500 in this case).\n",
    "    return padded_review\n",
    "\n",
    "# Do prediction on user input\n",
    "# e.g. Input = A movie review in text format (e.g. \"This movie was fantastic! I loved it.\")\n",
    "# e.g. Output = Positive or Negative\n",
    "def predict_sentiment(review):\n",
    "    preprocessed_input=preprocess_text(review)\n",
    "    prediction=simple_rnn_model.predict(preprocessed_input)\n",
    "    sentiment = 'Positive' if prediction[0][0] > 0.5 else 'Negative'\n",
    "    return sentiment, prediction[0][0]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff285051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step\n",
      "Review: This movie was fantastic! The acting was great and the plot was thrilling.\n",
      "Sentiment: Positive\n",
      "Prediction Score: 0.5977053642272949\n"
     ]
    }
   ],
   "source": [
    "# Step 7 - Do prediction with a sample user input\n",
    "example_review = \"This movie was fantastic! The acting was great and the plot was thrilling.\"\n",
    "sentiment,score=predict_sentiment(example_review)\n",
    "\n",
    "print(f'Review: {example_review}')\n",
    "print(f'Sentiment: {sentiment}')\n",
    "print(f'Prediction Score: {score}') # Probability score indicating how positive the review is (closer to 1 means more positive, closer to 0 means more negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d077d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28a0750",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
