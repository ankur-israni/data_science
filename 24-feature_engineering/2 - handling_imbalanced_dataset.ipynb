{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "109cb8dd",
   "metadata": {},
   "source": [
    "**Classifiation Dataset**\n",
    "A Classification dataset is one wherein the dataset can be split into one or more categories.\n",
    "Example #1: Survey Data(yes, no answers)\n",
    "\n",
    "**Classifiation Problem**\n",
    "A Classification Problem is one where we are trying to find some metric (mean, meadian, mode, standard deviation, etc) on a Classification Dataset\n",
    "\n",
    "**Imbalanced Dataset**\n",
    "An imbalanced dataset is one where data is leaning towards one classification of data.\n",
    "e.g. Survey data(yes, no answers) has 900 yes and 100 no.\n",
    "\n",
    "The problem with this Imbalance is that the model we will create will be biased towards this Imbalance(towards Yes based answers).\n",
    "\n",
    "The solution is to fix this imbalance. There are two technique availaible\n",
    "1) **Up Sampling**\n",
    "    Up Sampling means increasing the number of data points in a dataset to make a more balanced dataset.\n",
    "    Assume in a sample data set you have 900 'No Fraud' samples and 100 'Fraud' samples. This is an imbalanced dataset.\n",
    "    After Up Sampling techniques applied, you would have 900 'No Fraud' and 900 'Fraud' in the dataset making it a balanced dataset.\n",
    "\n",
    "    **Disadvantage of this technique**\n",
    "    - This technique of UpSampling adds th newly created datapoints on top of the previous datapoints - this does NOT add VARIANCE between datapoints.\n",
    "    - This problem is solved by using SMOTE oversampling.\n",
    "\n",
    "\n",
    "2) **Down Sampling**\n",
    "    Down Sampling means decreasing the number of data points in a dataset to make a more balanced dataset.\n",
    "    Assume in a sample data set you have 900 'No Fraud' samples and 100 'Fraud' samples. This is an imbalanced dataset.\n",
    "    After Down Sampling techniques applied, you would have 100 'No Fraud' and 100 'Fraud' in the dataset making it a balanced dataset.\n",
    "\n",
    "    **Disadvantage of Down Sampling.**\n",
    "    Down Sampling is bad because we are losing data points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3fd911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6964691855978616\n",
      "class Zeroes= 900 class Ones= 100\n",
      "class_Zeroes shape= (900, 3)\n",
      "class_Ones shape= (100, 3)\n",
      "imbalanced_data shape= (1000, 3)\n",
      "count of 0's and 1's in the target column:\n",
      "target\n",
      "0    900\n",
      "1    100\n",
      "Name: count, dtype: int64\n",
      " ******************* UP-SAMPLING ****************** \n",
      "df_minority_up_sampled shape= (900, 3)\n",
      "upsampled_data shape= (1800, 3)\n",
      "Final upsampled data\n",
      "target\n",
      "0    900\n",
      "1    900\n",
      "Name: count, dtype: int64\n",
      " ******************* DOWN-SAMPLING ****************** \n",
      "df_majority_down_sampled shape= (100, 3)\n",
      "downsampled_data shape= (200, 3)\n",
      "Final downsampled data\n",
      "target\n",
      "0    100\n",
      "1    100\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set a random seed with any number(here 123). This will ensure reproducibility - will produce the same random numbers each time the code is run.\n",
    "np.random.seed(123)\n",
    "\n",
    "# This will generate the random number.\n",
    "print(np.random.rand())\n",
    "\n",
    "# ################### CREATE AN IMBALANCED DATASET ####################\n",
    "# CREATING AN IMBALANCED DATASET of class_Zeroes(900 total) zeros and class_Ones(100 total) ones\n",
    "# Create a DataFrame with two classes(class_Zeroes = 900 Zeroes and class_Ones = 100 Ones - Imbalanced Dataset)\n",
    "total_samples = 1000\n",
    "ratio = 0.9\n",
    "class_Zeroes = int(total_samples * ratio) #900 samples\n",
    "class_Ones = total_samples - class_Zeroes #100 samples\n",
    "\n",
    "print(\"class Zeroes=\",class_Zeroes, \"class Ones=\", class_Ones)\n",
    "\n",
    "# Create the imbalanced dataset\n",
    "# target': [0]* class_Zeroes means create '0' repeated class_Zeroes times (here 900 times)\n",
    "# np.random.normal() : Means Normal Distribution / Gaussian Distribution\n",
    "# loc = 1 menas mean = 1 for this distribution\n",
    "# scale = 1.0 means standard deviation = 1.0 for this distribution\n",
    "# This dataframe has 3 colums: feature1, feature2 and target\n",
    "# feature1 = random numbers from normal distribution with mean=1, stddev=1.0, \n",
    "# feature2 = random numbers from normal distribution with mean=1, stddev=1.0\n",
    "# target = 0 repeated 900 times\n",
    "class_Zeroes = pd.DataFrame({'feature1': np.random.normal(loc=1, scale=1.0, size=class_Zeroes),\n",
    "                             'feature2': np.random.normal(loc=1, scale=1.0, size=class_Zeroes),\n",
    "                             'target': [0]* class_Zeroes}) \n",
    "\n",
    "class_Ones = pd.DataFrame({'feature1': np.random.normal(loc=2, scale=1.0, size=class_Ones),\n",
    "                           'feature2': np.random.normal(loc=2, scale=1.0, size=class_Ones),\n",
    "                           'target': [1]* class_Ones})\n",
    "\n",
    "\n",
    "\n",
    "print(\"class_Zeroes shape=\", class_Zeroes.shape)\n",
    "print(\"class_Ones shape=\", class_Ones.shape)\n",
    "\n",
    "# Combine the two dataframes (class_Zeroes and class_Ones) to create the imbalanced dataset\n",
    "# ignore_index=True is used to reset the index of the new dataframe, if you dont do that then the index values from the origional dataframes will be retained and you could possibly have duplicate index values.\n",
    "imbalanced_data = pd.concat([class_Zeroes, class_Ones], ignore_index=True)\n",
    "\n",
    "print(\"imbalanced_data shape=\", imbalanced_data.shape) #1000 rows, 3 columns\n",
    "\n",
    "# Print the counts of each class in the target column\n",
    "print(\"count of 0's and 1's in the target column:\")\n",
    "print(imbalanced_data['target'].value_counts())\n",
    "\n",
    "\n",
    "# ################### UP-SAMPLING ####################\n",
    "print(\" ******************* UP-SAMPLING ****************** \")\n",
    "# Create two dataframes, one for each classification = df_majority (for class 0) and df_minority (for class 1)\n",
    "# Put all values from imbalanced_data where target column = 0 into df_majority\n",
    "# Put all values from imbalanced_data where target column = 1 into df_minority\n",
    "df_majority = imbalanced_data[imbalanced_data.target == 0]\n",
    "df_minority = imbalanced_data[imbalanced_data.target == 1]\n",
    "df_minority.shape #100 rows, 3 columns\n",
    "df_majority.shape #900 rows, 3 columns\n",
    "\n",
    "# Resample the minority class with replacement to match the number of samples in the majority class\n",
    "# df_majority has 900 rows, df_minority has 100 rows, we will UP-SAMPLE df_minority to have 900 rows\n",
    "from sklearn.utils import resample\n",
    "df_minority_up_sampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=len(df_majority),    # to match majority class, total length of majority class\n",
    "                                 random_state=123) # reproducible results\n",
    "\n",
    "print(\"df_minority_up_sampled shape=\", df_minority_up_sampled.shape) #900 rows, 3 columns, previous size was 100 rows, 3 columns\n",
    "\n",
    "# Combine the majority class with the upsampled minority class\n",
    "# df_majority has 900 rows, df_minority_up_sampled has 900 rows, total 1800 rows\n",
    "upsampled_data = pd.concat([df_majority, df_minority_up_sampled], ignore_index=True)\n",
    "print(\"upsampled_data shape=\", upsampled_data.shape) #1800 rows, 3 columns\n",
    "\n",
    "# Print the counts of each class in the target column after upsampling\n",
    "print(\"Final upsampled data\")\n",
    "print(upsampled_data['target'].value_counts()) #900 of each class (0's and 1's)\n",
    "\n",
    "# ################### DOWN-SAMPLING ####################\n",
    "print(\" ******************* DOWN-SAMPLING ****************** \")\n",
    "# Create two dataframes, one for each classification = df_majority (for class 0) and df_minority (for class 1)\n",
    "# Put all values from imbalanced_data where target column = 0 into df_majority\n",
    "# Put all values from imbalanced_data where target column = 1 into df_minority\n",
    "df_majority = imbalanced_data[imbalanced_data.target == 0]\n",
    "df_minority = imbalanced_data[imbalanced_data.target == 1]\n",
    "df_minority.shape #100 rows, 3 columns\n",
    "df_majority.shape #900 rows, 3 columns\n",
    "\n",
    "# Resample the minority class with replacement to match the number of samples in the majority class\n",
    "# df_majority has 900 rows, df_minority has 100 rows, we will UP-SAMPLE df_minority to have 900 rows\n",
    "from sklearn.utils import resample\n",
    "df_majority_down_sampled = resample(df_majority, \n",
    "                                 replace=False,     # Replace is false because we are down-sampling,we want to delete the rows from 900(majority class) to 100(minority class)\n",
    "                                 n_samples=len(df_minority),    # to match minority class, total length of minority class\n",
    "                                 random_state=123) # reproducible results\n",
    "\n",
    "print(\"df_majority_down_sampled shape=\", df_majority_down_sampled.shape) #100 rows, 3 columns, previous size was 900 rows, 3 columns\n",
    "\n",
    "# Combine the majority class with the upsampled minority class\n",
    "# df_majority has 900 rows, df_minority_up_sampled has 900 rows, total 1800 rows\n",
    "downsampled_data = pd.concat([df_majority_down_sampled, df_minority], ignore_index=True)\n",
    "print(\"downsampled_data shape=\", downsampled_data.shape) #200 rows, 3 columns\n",
    "\n",
    "# Print the counts of each class in the target column after downsampling\n",
    "print(\"Final downsampled data\")\n",
    "print(downsampled_data['target'].value_counts()) #100 of each class (0's and 1's)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
